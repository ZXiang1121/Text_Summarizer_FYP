{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Source Modelling with BBC News Dataset\n",
    "\n",
    "For comparison of Summary in different aspects/topics.\n",
    "\n",
    "Understand LLM Strengths and Weaknesses:\n",
    "\n",
    "- **Identify domain-specific strengths**: Different LLMs are trained on different datasets and may excel in different domains. Comparing summaries across topics can help you identify which LLM performs best in a specific area relevant to your needs.\n",
    "\n",
    "- **Uncover biases and limitations**: LLMs can inherit biases from their training data. Comparing summaries can help you identify potential biases and limitations in different models, allowing you to choose the one with the least bias for your task.\n",
    "\n",
    "- **Evaluate factual accuracy**: Some LLMs prioritize fluency over factual accuracy, while others excel at fact-checking. Comparing summaries can help you assess the factual accuracy of each LLM and choose the one that best suits your need for reliable information.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. **Install & Import Necessary Libraries**\n",
    "\n",
    "\n",
    "2. **Load Dataset (BBC News)** : Pick 5 rows of data per aspects\n",
    "\n",
    "\n",
    "3. **OpenAI Topic Summary Generation**\n",
    "\n",
    "\n",
    "4. **Google Topic Summary Generation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install & Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Zhang\n",
      "[nltk_data]     Xiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zhang\n",
      "[nltk_data]     Xiang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sys.path.append('../')\n",
    "from helper.SummarizationMetrics import SummarizationMetrics\n",
    "from helper.chatgpt_automation import ChatGPTAutomation, split_text_into_chunks\n",
    "from helper.bard_automation import BardAutomation, split_text_into_chunks\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from langchain import LLMChain, HuggingFacePipeline\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.llms import VertexAI\n",
    "import google.generativeai as genai\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from scipy.signal import argrelextrema\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from os import environ\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from helper.SummarizationMetrics import SummarizationMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset (BBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_path</th>\n",
       "      <th>Articles</th>\n",
       "      <th>Summaries</th>\n",
       "      <th>transcript</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Cuba winds back economic clock..Fidel Castro's...</td>\n",
       "      <td>Fidel Castro's decision to ban all cash transa...</td>\n",
       "      <td>Cuba winds back economic clock..Fidel Castro's...</td>\n",
       "      <td>Fidel Castro's decision to ban all cash transa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politics</td>\n",
       "      <td>Blair looks to election campaign..Tony Blair's...</td>\n",
       "      <td>There was little in terms of concrete proposal...</td>\n",
       "      <td>Blair looks to election campaign..Tony Blair's...</td>\n",
       "      <td>There was little in terms of concrete proposal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>New York rockers top talent poll..New York ele...</td>\n",
       "      <td>New York electro-rock group The Bravery have c...</td>\n",
       "      <td>New York rockers top talent poll..New York ele...</td>\n",
       "      <td>New York electro-rock group The Bravery have c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politics</td>\n",
       "      <td>Terror suspects face house arrest..UK citizens...</td>\n",
       "      <td>British citizens are being included in the cha...</td>\n",
       "      <td>Terror suspects face house arrest..UK citizens...</td>\n",
       "      <td>British citizens are being included in the cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>'No more concessions' on terror..Charles Clark...</td>\n",
       "      <td>On Monday, MPs voted 272-219 in favour of the ...</td>\n",
       "      <td>'No more concessions' on terror..Charles Clark...</td>\n",
       "      <td>On Monday, MPs voted 272-219 in favour of the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       File_path                                           Articles  \\\n",
       "0       business  Cuba winds back economic clock..Fidel Castro's...   \n",
       "1       politics  Blair looks to election campaign..Tony Blair's...   \n",
       "2  entertainment  New York rockers top talent poll..New York ele...   \n",
       "3       politics  Terror suspects face house arrest..UK citizens...   \n",
       "4       politics  'No more concessions' on terror..Charles Clark...   \n",
       "\n",
       "                                           Summaries  \\\n",
       "0  Fidel Castro's decision to ban all cash transa...   \n",
       "1  There was little in terms of concrete proposal...   \n",
       "2  New York electro-rock group The Bravery have c...   \n",
       "3  British citizens are being included in the cha...   \n",
       "4  On Monday, MPs voted 272-219 in favour of the ...   \n",
       "\n",
       "                                          transcript  \\\n",
       "0  Cuba winds back economic clock..Fidel Castro's...   \n",
       "1  Blair looks to election campaign..Tony Blair's...   \n",
       "2  New York rockers top talent poll..New York ele...   \n",
       "3  Terror suspects face house arrest..UK citizens...   \n",
       "4  'No more concessions' on terror..Charles Clark...   \n",
       "\n",
       "                                             summary  \n",
       "0  Fidel Castro's decision to ban all cash transa...  \n",
       "1  There was little in terms of concrete proposal...  \n",
       "2  New York electro-rock group The Bravery have c...  \n",
       "3  British citizens are being included in the cha...  \n",
       "4  On Monday, MPs voted 272-219 in favour of the ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_train_df = pd.read_excel(\"../Data/newsbbc_train.xlsx\")\n",
    "\n",
    "bbc_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File_path\n",
       "business         5\n",
       "politics         5\n",
       "entertainment    5\n",
       "sport            5\n",
       "tech             5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_train_df = bbc_train_df.groupby('File_path').head(5)\n",
    "bbc_train_df['File_path'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_path</th>\n",
       "      <th>Articles</th>\n",
       "      <th>Summaries</th>\n",
       "      <th>transcript</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Cuba winds back economic clock..Fidel Castro's...</td>\n",
       "      <td>Fidel Castro's decision to ban all cash transa...</td>\n",
       "      <td>Cuba winds back economic clock..Fidel Castro's...</td>\n",
       "      <td>Fidel Castro's decision to ban all cash transa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politics</td>\n",
       "      <td>Blair looks to election campaign..Tony Blair's...</td>\n",
       "      <td>There was little in terms of concrete proposal...</td>\n",
       "      <td>Blair looks to election campaign..Tony Blair's...</td>\n",
       "      <td>There was little in terms of concrete proposal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>New York rockers top talent poll..New York ele...</td>\n",
       "      <td>New York electro-rock group The Bravery have c...</td>\n",
       "      <td>New York rockers top talent poll..New York ele...</td>\n",
       "      <td>New York electro-rock group The Bravery have c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politics</td>\n",
       "      <td>Terror suspects face house arrest..UK citizens...</td>\n",
       "      <td>British citizens are being included in the cha...</td>\n",
       "      <td>Terror suspects face house arrest..UK citizens...</td>\n",
       "      <td>British citizens are being included in the cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>'No more concessions' on terror..Charles Clark...</td>\n",
       "      <td>On Monday, MPs voted 272-219 in favour of the ...</td>\n",
       "      <td>'No more concessions' on terror..Charles Clark...</td>\n",
       "      <td>On Monday, MPs voted 272-219 in favour of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>Howard denies split over ID cards..Michael How...</td>\n",
       "      <td>Michael Howard has denied his shadow cabinet w...</td>\n",
       "      <td>Howard denies split over ID cards..Michael How...</td>\n",
       "      <td>Michael Howard has denied his shadow cabinet w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sport</td>\n",
       "      <td>Chelsea denied by James heroics..A brave defen...</td>\n",
       "      <td>Chelsea were now looking more like Premiership...</td>\n",
       "      <td>Chelsea denied by James heroics..A brave defen...</td>\n",
       "      <td>Chelsea were now looking more like Premiership...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>politics</td>\n",
       "      <td>Guantanamo man 'suing government'..A British t...</td>\n",
       "      <td>He said he was sent there after being interrog...</td>\n",
       "      <td>Guantanamo man 'suing government'..A British t...</td>\n",
       "      <td>He said he was sent there after being interrog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>business</td>\n",
       "      <td>Could Yukos be a blessing in disguise?..Other ...</td>\n",
       "      <td>But it argues that more rigorous tax policing ...</td>\n",
       "      <td>Could Yukos be a blessing in disguise?..Other ...</td>\n",
       "      <td>But it argues that more rigorous tax policing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>business</td>\n",
       "      <td>Asian quake hits European shares..Shares in Eu...</td>\n",
       "      <td>The unfolding scale of the disaster in south A...</td>\n",
       "      <td>Asian quake hits European shares..Shares in Eu...</td>\n",
       "      <td>The unfolding scale of the disaster in south A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tech</td>\n",
       "      <td>The year search became personal..The odds are ...</td>\n",
       "      <td>Web users face a plethora of choices as each c...</td>\n",
       "      <td>The year search became personal..The odds are ...</td>\n",
       "      <td>Web users face a plethora of choices as each c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tech</td>\n",
       "      <td>Doors open at biggest gadget fair..Thousands o...</td>\n",
       "      <td>He said the products which will be making wave...</td>\n",
       "      <td>Doors open at biggest gadget fair..Thousands o...</td>\n",
       "      <td>He said the products which will be making wave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tech</td>\n",
       "      <td>File-swappers ready new network..Legal attacks...</td>\n",
       "      <td>Sloncek said that currently only a Windows ver...</td>\n",
       "      <td>File-swappers ready new network..Legal attacks...</td>\n",
       "      <td>Sloncek said that currently only a Windows ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tech</td>\n",
       "      <td>Toxic web links help virus spread..Virus write...</td>\n",
       "      <td>A Windows virus called Bofra is turning infect...</td>\n",
       "      <td>Toxic web links help virus spread..Virus write...</td>\n",
       "      <td>A Windows virus called Bofra is turning infect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tech</td>\n",
       "      <td>Mobiles 'not media players yet'..Mobiles are n...</td>\n",
       "      <td>The service uses 3GP technology, one of the st...</td>\n",
       "      <td>Mobiles 'not media players yet'..Mobiles are n...</td>\n",
       "      <td>The service uses 3GP technology, one of the st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sport</td>\n",
       "      <td>European medal chances improve..What have the ...</td>\n",
       "      <td>Diane Allahgreen has been our best hurdler for...</td>\n",
       "      <td>European medal chances improve..What have the ...</td>\n",
       "      <td>Diane Allahgreen has been our best hurdler for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>business</td>\n",
       "      <td>Disaster claims 'less than $10bn'..Insurers ha...</td>\n",
       "      <td>The impact on US insurance companies is not ex...</td>\n",
       "      <td>Disaster claims 'less than $10bn'..Insurers ha...</td>\n",
       "      <td>The impact on US insurance companies is not ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Franz Ferdinand's art school lesson..Scottish ...</td>\n",
       "      <td>The buzz about the band soon spread around the...</td>\n",
       "      <td>Franz Ferdinand's art school lesson..Scottish ...</td>\n",
       "      <td>The buzz about the band soon spread around the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sport</td>\n",
       "      <td>Man Utd stroll to Cup win..Wayne Rooney made a...</td>\n",
       "      <td>But there was nothing Martyn could do when Uni...</td>\n",
       "      <td>Man Utd stroll to Cup win..Wayne Rooney made a...</td>\n",
       "      <td>But there was nothing Martyn could do when Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Stars shine on Bafta red carpet..Hollywood sta...</td>\n",
       "      <td>Keanu Reeves, who presented the best actress a...</td>\n",
       "      <td>Stars shine on Bafta red carpet..Hollywood sta...</td>\n",
       "      <td>Keanu Reeves, who presented the best actress a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sport</td>\n",
       "      <td>Paris promise raises Welsh hopes..Has there be...</td>\n",
       "      <td>But since they threw off the shackles against ...</td>\n",
       "      <td>Paris promise raises Welsh hopes..Has there be...</td>\n",
       "      <td>But since they threw off the shackles against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Rapper Kanye West's shrewd soul..US hip-hop st...</td>\n",
       "      <td>Leaving his Chicago art school after only one ...</td>\n",
       "      <td>Rapper Kanye West's shrewd soul..US hip-hop st...</td>\n",
       "      <td>Leaving his Chicago art school after only one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Redford's vision of Sundance..Despite sporting...</td>\n",
       "      <td>Redford wanted Sundance to be a platform for i...</td>\n",
       "      <td>Redford's vision of Sundance..Despite sporting...</td>\n",
       "      <td>Redford wanted Sundance to be a platform for i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>business</td>\n",
       "      <td>Water firm Suez in Argentina row..A conflict b...</td>\n",
       "      <td>The government has rejected the 60% rise and w...</td>\n",
       "      <td>Water firm Suez in Argentina row..A conflict b...</td>\n",
       "      <td>The government has rejected the 60% rise and w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>sport</td>\n",
       "      <td>Lions blow to World Cup winners..British and I...</td>\n",
       "      <td>But Woodward added: \"The key thing that I want...</td>\n",
       "      <td>Lions blow to World Cup winners..British and I...</td>\n",
       "      <td>But Woodward added: \"The key thing that I want...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        File_path                                           Articles  \\\n",
       "0        business  Cuba winds back economic clock..Fidel Castro's...   \n",
       "1        politics  Blair looks to election campaign..Tony Blair's...   \n",
       "2   entertainment  New York rockers top talent poll..New York ele...   \n",
       "3        politics  Terror suspects face house arrest..UK citizens...   \n",
       "4        politics  'No more concessions' on terror..Charles Clark...   \n",
       "5        politics  Howard denies split over ID cards..Michael How...   \n",
       "6           sport  Chelsea denied by James heroics..A brave defen...   \n",
       "7        politics  Guantanamo man 'suing government'..A British t...   \n",
       "13       business  Could Yukos be a blessing in disguise?..Other ...   \n",
       "14       business  Asian quake hits European shares..Shares in Eu...   \n",
       "15           tech  The year search became personal..The odds are ...   \n",
       "17           tech  Doors open at biggest gadget fair..Thousands o...   \n",
       "18           tech  File-swappers ready new network..Legal attacks...   \n",
       "19           tech  Toxic web links help virus spread..Virus write...   \n",
       "20           tech  Mobiles 'not media players yet'..Mobiles are n...   \n",
       "24          sport  European medal chances improve..What have the ...   \n",
       "25       business  Disaster claims 'less than $10bn'..Insurers ha...   \n",
       "28  entertainment  Franz Ferdinand's art school lesson..Scottish ...   \n",
       "33          sport  Man Utd stroll to Cup win..Wayne Rooney made a...   \n",
       "37  entertainment  Stars shine on Bafta red carpet..Hollywood sta...   \n",
       "40          sport  Paris promise raises Welsh hopes..Has there be...   \n",
       "46  entertainment  Rapper Kanye West's shrewd soul..US hip-hop st...   \n",
       "47  entertainment  Redford's vision of Sundance..Despite sporting...   \n",
       "50       business  Water firm Suez in Argentina row..A conflict b...   \n",
       "75          sport  Lions blow to World Cup winners..British and I...   \n",
       "\n",
       "                                            Summaries  \\\n",
       "0   Fidel Castro's decision to ban all cash transa...   \n",
       "1   There was little in terms of concrete proposal...   \n",
       "2   New York electro-rock group The Bravery have c...   \n",
       "3   British citizens are being included in the cha...   \n",
       "4   On Monday, MPs voted 272-219 in favour of the ...   \n",
       "5   Michael Howard has denied his shadow cabinet w...   \n",
       "6   Chelsea were now looking more like Premiership...   \n",
       "7   He said he was sent there after being interrog...   \n",
       "13  But it argues that more rigorous tax policing ...   \n",
       "14  The unfolding scale of the disaster in south A...   \n",
       "15  Web users face a plethora of choices as each c...   \n",
       "17  He said the products which will be making wave...   \n",
       "18  Sloncek said that currently only a Windows ver...   \n",
       "19  A Windows virus called Bofra is turning infect...   \n",
       "20  The service uses 3GP technology, one of the st...   \n",
       "24  Diane Allahgreen has been our best hurdler for...   \n",
       "25  The impact on US insurance companies is not ex...   \n",
       "28  The buzz about the band soon spread around the...   \n",
       "33  But there was nothing Martyn could do when Uni...   \n",
       "37  Keanu Reeves, who presented the best actress a...   \n",
       "40  But since they threw off the shackles against ...   \n",
       "46  Leaving his Chicago art school after only one ...   \n",
       "47  Redford wanted Sundance to be a platform for i...   \n",
       "50  The government has rejected the 60% rise and w...   \n",
       "75  But Woodward added: \"The key thing that I want...   \n",
       "\n",
       "                                           transcript  \\\n",
       "0   Cuba winds back economic clock..Fidel Castro's...   \n",
       "1   Blair looks to election campaign..Tony Blair's...   \n",
       "2   New York rockers top talent poll..New York ele...   \n",
       "3   Terror suspects face house arrest..UK citizens...   \n",
       "4   'No more concessions' on terror..Charles Clark...   \n",
       "5   Howard denies split over ID cards..Michael How...   \n",
       "6   Chelsea denied by James heroics..A brave defen...   \n",
       "7   Guantanamo man 'suing government'..A British t...   \n",
       "13  Could Yukos be a blessing in disguise?..Other ...   \n",
       "14  Asian quake hits European shares..Shares in Eu...   \n",
       "15  The year search became personal..The odds are ...   \n",
       "17  Doors open at biggest gadget fair..Thousands o...   \n",
       "18  File-swappers ready new network..Legal attacks...   \n",
       "19  Toxic web links help virus spread..Virus write...   \n",
       "20  Mobiles 'not media players yet'..Mobiles are n...   \n",
       "24  European medal chances improve..What have the ...   \n",
       "25  Disaster claims 'less than $10bn'..Insurers ha...   \n",
       "28  Franz Ferdinand's art school lesson..Scottish ...   \n",
       "33  Man Utd stroll to Cup win..Wayne Rooney made a...   \n",
       "37  Stars shine on Bafta red carpet..Hollywood sta...   \n",
       "40  Paris promise raises Welsh hopes..Has there be...   \n",
       "46  Rapper Kanye West's shrewd soul..US hip-hop st...   \n",
       "47  Redford's vision of Sundance..Despite sporting...   \n",
       "50  Water firm Suez in Argentina row..A conflict b...   \n",
       "75  Lions blow to World Cup winners..British and I...   \n",
       "\n",
       "                                              summary  \n",
       "0   Fidel Castro's decision to ban all cash transa...  \n",
       "1   There was little in terms of concrete proposal...  \n",
       "2   New York electro-rock group The Bravery have c...  \n",
       "3   British citizens are being included in the cha...  \n",
       "4   On Monday, MPs voted 272-219 in favour of the ...  \n",
       "5   Michael Howard has denied his shadow cabinet w...  \n",
       "6   Chelsea were now looking more like Premiership...  \n",
       "7   He said he was sent there after being interrog...  \n",
       "13  But it argues that more rigorous tax policing ...  \n",
       "14  The unfolding scale of the disaster in south A...  \n",
       "15  Web users face a plethora of choices as each c...  \n",
       "17  He said the products which will be making wave...  \n",
       "18  Sloncek said that currently only a Windows ver...  \n",
       "19  A Windows virus called Bofra is turning infect...  \n",
       "20  The service uses 3GP technology, one of the st...  \n",
       "24  Diane Allahgreen has been our best hurdler for...  \n",
       "25  The impact on US insurance companies is not ex...  \n",
       "28  The buzz about the band soon spread around the...  \n",
       "33  But there was nothing Martyn could do when Uni...  \n",
       "37  Keanu Reeves, who presented the best actress a...  \n",
       "40  But since they threw off the shackles against ...  \n",
       "46  Leaving his Chicago art school after only one ...  \n",
       "47  Redford wanted Sundance to be a platform for i...  \n",
       "50  The government has rejected the 60% rise and w...  \n",
       "75  But Woodward added: \"The key thing that I want...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load File that contain API Key\n",
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve API Key\n",
    "openai_api_key = environ.get(\"OPENAI_API_KEY\")\n",
    "gcp_api_key = environ.get(\"GCP_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=gcp_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/text-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 (Legacy)',\n",
      "      description='A legacy model that understands text and generates text as an output',\n",
      "      input_token_limit=8196,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
      "      temperature=0.7,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "# List Model Available on genai for Google.\n",
    "for model in genai.list_models():\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI Topic Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>max_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>128000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               models  max_tokens\n",
       "0  gpt-4-0125-preview      128000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Models\n",
    "# Test 1-2 models at a time to avoid Kernel stopping.\n",
    "openai_models = {\n",
    "    \"models\": [\n",
    "        # \"gpt-3.5-turbo-1106\",\n",
    "        # \"gpt-3.5-turbo-0125\",\n",
    "        # \"gpt-4\",\n",
    "        # \"gpt-4-1106-preview\",\n",
    "        \"gpt-4-0125-preview\",\n",
    "\n",
    "    ],\n",
    "    \"max_tokens\": [\n",
    "        # 16385,\n",
    "        # 16385,\n",
    "        # 8192,\n",
    "        # 128000,\n",
    "        128000,\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_openai_models = pd.DataFrame(openai_models)\n",
    "df_openai_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>method</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>topic</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>transcript</th>\n",
       "      <th>original summary</th>\n",
       "      <th>summary</th>\n",
       "      <th>grammar</th>\n",
       "      <th>readability</th>\n",
       "      <th>rouge</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>prompt</th>\n",
       "      <th>temperature</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, method, max_tokens, topic, num_tokens, transcript, original summary, summary, grammar, readability, rouge, bert_score, time_taken, prompt, temperature, bleu]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores = pd.DataFrame(columns=['model', 'method', 'max_tokens', 'topic' ,'num_tokens' ,'transcript', 'original summary' ,'summary', 'grammar', 'readability', 'rouge', 'bert_score', 'time_taken', 'prompt', 'temperature'])\n",
    "df_scores\n",
    "\n",
    "# Run Read Excel when adding more data into the DataFrame\n",
    "df_scores = pd.read_excel(\"./result/closed_source_model_topics_comparison.xlsx\")\n",
    "df_scores.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4-0125-preview\n",
      "Number of tokens: 762\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 52.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.24 seconds, 0.45 sentences/sec\n",
      "Number of tokens: 702\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.62 seconds, 0.18 sentences/sec\n",
      "Number of tokens: 718\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.80 seconds, 0.26 sentences/sec\n",
      "Number of tokens: 671\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 52.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.71 seconds, 0.21 sentences/sec\n",
      "Number of tokens: 642\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 71.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.07 seconds, 0.25 sentences/sec\n",
      "Number of tokens: 634\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 500.10it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.86 seconds, 0.54 sentences/sec\n",
      "Number of tokens: 726\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 52.47it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.10 seconds, 0.32 sentences/sec\n",
      "Number of tokens: 753\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 217.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 5.12 seconds, 0.20 sentences/sec\n",
      "Number of tokens: 875\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.41 seconds, 0.41 sentences/sec\n",
      "Number of tokens: 756\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 237.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.52 seconds, 0.40 sentences/sec\n",
      "Number of tokens: 707\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.30it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.21 seconds, 0.45 sentences/sec\n",
      "Number of tokens: 842\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.35 seconds, 0.43 sentences/sec\n",
      "Number of tokens: 829\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 466.81it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.58 seconds, 0.39 sentences/sec\n",
      "Number of tokens: 639\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.23it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.97 seconds, 0.51 sentences/sec\n",
      "Number of tokens: 787\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 499.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.99 seconds, 0.50 sentences/sec\n",
      "Number of tokens: 800\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 111.13it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.10 seconds, 0.48 sentences/sec\n",
      "Number of tokens: 838\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.70 seconds, 0.37 sentences/sec\n",
      "Number of tokens: 694\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.69 seconds, 0.27 sentences/sec\n",
      "Number of tokens: 712\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.12it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.16 seconds, 0.46 sentences/sec\n",
      "Number of tokens: 774\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.20it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.51 seconds, 0.40 sentences/sec\n",
      "Number of tokens: 805\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 227.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.79 seconds, 0.56 sentences/sec\n",
      "Number of tokens: 707\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 332.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.10 seconds, 0.48 sentences/sec\n",
      "Number of tokens: 636\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 213.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.61 seconds, 0.62 sentences/sec\n",
      "Number of tokens: 697\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 68.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.50 seconds, 0.40 sentences/sec\n",
      "Number of tokens: 594\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 398.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.68 seconds, 0.60 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "for model_index, model_row in df_openai_models.iterrows():\n",
    "    model_name = model_row[\"models\"]\n",
    "    print(model_name)\n",
    "\n",
    "    temperature = 0\n",
    "\n",
    "    llm = ChatOpenAI(temperature=temperature, model_name=model_name, openai_api_key=openai_api_key)\n",
    "    \n",
    "    prompt_template = \"\"\"Write a concise summary of the following: {text}\"\"\"\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "    for index, row in bbc_train_df.iterrows():\n",
    "        method = \"MapReduce\"\n",
    "\n",
    "        # get the summary\n",
    "        start_time = time.time()\n",
    "        num_tokens = llm.get_num_tokens(row['transcript'])\n",
    "        print(\"Number of tokens:\", num_tokens)\n",
    "\n",
    "        max_tokens = model_row[\"max_tokens\"]\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=max_tokens-100, chunk_overlap=100)\n",
    "        docs = text_splitter.create_documents([row['transcript']])\n",
    "        print(\"Number of chunks:\", len(docs))\n",
    "\n",
    "        # break\n",
    "        summary_chain = load_summarize_chain(llm=llm, chain_type='map_reduce', token_max=max_tokens , map_prompt=PROMPT)\n",
    "        summary = summary_chain.run(docs)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        metrics = SummarizationMetrics(row['summary'], summary)\n",
    "\n",
    "        new_result = {\n",
    "            'model': model_name,\n",
    "            'method': method,\n",
    "            'max_tokens': max_tokens,\n",
    "            'topic': row[\"File_path\"],\n",
    "            'transcript': row['transcript'],\n",
    "            'original summary': row['summary'],\n",
    "            'summary': summary,\n",
    "            'rouge': metrics.rouge_scores(),\n",
    "            'bert_score': metrics.bert_score(),\n",
    "            'bleu': metrics.bleu_score(),\n",
    "            'time_taken': elapsed_time,\n",
    "            'grammar': metrics.grammar_check(),\n",
    "            'readability': metrics.readability_index(),\n",
    "            'num_tokens': num_tokens,\n",
    "            'prompt': prompt_template,\n",
    "            'temperature': temperature\n",
    "        }\n",
    "\n",
    "\n",
    "        new_row = pd.DataFrame([new_result])\n",
    "\n",
    "        df_scores = pd.concat([df_scores, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>method</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>topic</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>transcript</th>\n",
       "      <th>original summary</th>\n",
       "      <th>summary</th>\n",
       "      <th>grammar</th>\n",
       "      <th>readability</th>\n",
       "      <th>rouge</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>prompt</th>\n",
       "      <th>temperature</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16385</td>\n",
       "      <td>business</td>\n",
       "      <td>762</td>\n",
       "      <td>Cuba winds back economic clock..Fidel Castro's...</td>\n",
       "      <td>Fidel Castro's decision to ban all cash transa...</td>\n",
       "      <td>Cuba has imposed a 10% tax on conversions betw...</td>\n",
       "      <td>[]</td>\n",
       "      <td>100 words required.</td>\n",
       "      <td>[{'rouge-1': {'r': 0.17006802721088435, 'p': 0...</td>\n",
       "      <td>(tensor([0.9130]), tensor([0.8402]), tensor([0...</td>\n",
       "      <td>3.803998</td>\n",
       "      <td>Write a concise summary of the following: {text}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16385</td>\n",
       "      <td>politics</td>\n",
       "      <td>702</td>\n",
       "      <td>Blair looks to election campaign..Tony Blair's...</td>\n",
       "      <td>There was little in terms of concrete proposal...</td>\n",
       "      <td>Tony Blair's recent speech marked the start of...</td>\n",
       "      <td>[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...</td>\n",
       "      <td>100 words required.</td>\n",
       "      <td>[{'rouge-1': {'r': 0.17365269461077845, 'p': 0...</td>\n",
       "      <td>(tensor([0.8809]), tensor([0.8259]), tensor([0...</td>\n",
       "      <td>4.186849</td>\n",
       "      <td>Write a concise summary of the following: {text}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.016755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16385</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>718</td>\n",
       "      <td>New York rockers top talent poll..New York ele...</td>\n",
       "      <td>New York electro-rock group The Bravery have c...</td>\n",
       "      <td>The Bravery, a New York electro-rock group, to...</td>\n",
       "      <td>[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...</td>\n",
       "      <td>100 words required.</td>\n",
       "      <td>[{'rouge-1': {'r': 0.26666666666666666, 'p': 0...</td>\n",
       "      <td>(tensor([0.9043]), tensor([0.8542]), tensor([0...</td>\n",
       "      <td>2.662916</td>\n",
       "      <td>Write a concise summary of the following: {text}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16385</td>\n",
       "      <td>politics</td>\n",
       "      <td>671</td>\n",
       "      <td>Terror suspects face house arrest..UK citizens...</td>\n",
       "      <td>British citizens are being included in the cha...</td>\n",
       "      <td>The UK government is considering new measures ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>100 words required.</td>\n",
       "      <td>[{'rouge-1': {'r': 0.16265060240963855, 'p': 0...</td>\n",
       "      <td>(tensor([0.8905]), tensor([0.8307]), tensor([0...</td>\n",
       "      <td>3.348620</td>\n",
       "      <td>Write a concise summary of the following: {text}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16385</td>\n",
       "      <td>politics</td>\n",
       "      <td>642</td>\n",
       "      <td>'No more concessions' on terror..Charles Clark...</td>\n",
       "      <td>On Monday, MPs voted 272-219 in favour of the ...</td>\n",
       "      <td>Charles Clarke is standing firm on his anti-te...</td>\n",
       "      <td>[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...</td>\n",
       "      <td>100 words required.</td>\n",
       "      <td>[{'rouge-1': {'r': 0.2751677852348993, 'p': 0....</td>\n",
       "      <td>(tensor([0.8944]), tensor([0.8478]), tensor([0...</td>\n",
       "      <td>2.810315</td>\n",
       "      <td>Write a concise summary of the following: {text}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>128000</td>\n",
       "      <td>sport</td>\n",
       "      <td>805</td>\n",
       "      <td>Paris promise raises Welsh hopes..Has there be...</td>\n",
       "      <td>But since they threw off the shackles against ...</td>\n",
       "      <td>The article highlights the rising expectations...</td>\n",
       "      <td>[]</td>\n",
       "      <td>score: 14.651290322580646, grade_level: '15'</td>\n",
       "      <td>[{'rouge-1': {'r': 0.17391304347826086, 'p': 0...</td>\n",
       "      <td>([tensor(0.8637)], [tensor(0.8236)], [tensor(0...</td>\n",
       "      <td>33.090430</td>\n",
       "      <td>Write a concise summary of the following: {text}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>128000</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>707</td>\n",
       "      <td>Rapper Kanye West's shrewd soul..US hip-hop st...</td>\n",
       "      <td>Leaving his Chicago art school after only one ...</td>\n",
       "      <td>Kanye West, a key figure in hip-hop, leads the...</td>\n",
       "      <td>[Offset 557, length 4, Rule ID: MORFOLOGIK_RUL...</td>\n",
       "      <td>score: 13.59773553719008, grade_level: '14'</td>\n",
       "      <td>[{'rouge-1': {'r': 0.2430939226519337, 'p': 0....</td>\n",
       "      <td>([tensor(0.8815)], [tensor(0.8397)], [tensor(0...</td>\n",
       "      <td>22.018759</td>\n",
       "      <td>Write a concise summary of the following: {text}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.034853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>128000</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>636</td>\n",
       "      <td>Redford's vision of Sundance..Despite sporting...</td>\n",
       "      <td>Redford wanted Sundance to be a platform for i...</td>\n",
       "      <td>Robert Redford founded the Sundance Film Festi...</td>\n",
       "      <td>[Offset 27, length 8, Rule ID: MORFOLOGIK_RULE...</td>\n",
       "      <td>score: 15.02, grade_level: '15'</td>\n",
       "      <td>[{'rouge-1': {'r': 0.16129032258064516, 'p': 0...</td>\n",
       "      <td>([tensor(0.8860)], [tensor(0.8411)], [tensor(0...</td>\n",
       "      <td>25.070732</td>\n",
       "      <td>Write a concise summary of the following: {text}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.025239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>128000</td>\n",
       "      <td>business</td>\n",
       "      <td>697</td>\n",
       "      <td>Water firm Suez in Argentina row..A conflict b...</td>\n",
       "      <td>The government has rejected the 60% rise and w...</td>\n",
       "      <td>The Argentine government is in a dispute with ...</td>\n",
       "      <td>[Offset 46, length 5, Rule ID: MORFOLOGIK_RULE...</td>\n",
       "      <td>score: 17.724833333333333, grade_level: '18'</td>\n",
       "      <td>[{'rouge-1': {'r': 0.2929936305732484, 'p': 0....</td>\n",
       "      <td>([tensor(0.8820)], [tensor(0.8585)], [tensor(0...</td>\n",
       "      <td>43.071345</td>\n",
       "      <td>Write a concise summary of the following: {text}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.064537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>gpt-4-0125-preview</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>128000</td>\n",
       "      <td>sport</td>\n",
       "      <td>594</td>\n",
       "      <td>Lions blow to World Cup winners..British and I...</td>\n",
       "      <td>But Woodward added: \"The key thing that I want...</td>\n",
       "      <td>Clive Woodward, coach of the British and Irish...</td>\n",
       "      <td>[Offset 250, length 9, Rule ID: MORFOLOGIK_RUL...</td>\n",
       "      <td>score: 15.618651162790702, grade_level: '16'</td>\n",
       "      <td>[{'rouge-1': {'r': 0.3105590062111801, 'p': 0....</td>\n",
       "      <td>([tensor(0.8841)], [tensor(0.8443)], [tensor(0...</td>\n",
       "      <td>30.502107</td>\n",
       "      <td>Write a concise summary of the following: {text}</td>\n",
       "      <td>0</td>\n",
       "      <td>0.031168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model     method  max_tokens          topic  num_tokens  \\\n",
       "0   gpt-3.5-turbo-0125  MapReduce       16385       business         762   \n",
       "1   gpt-3.5-turbo-0125  MapReduce       16385       politics         702   \n",
       "2   gpt-3.5-turbo-0125  MapReduce       16385  entertainment         718   \n",
       "3   gpt-3.5-turbo-0125  MapReduce       16385       politics         671   \n",
       "4   gpt-3.5-turbo-0125  MapReduce       16385       politics         642   \n",
       "..                 ...        ...         ...            ...         ...   \n",
       "70  gpt-4-0125-preview  MapReduce      128000          sport         805   \n",
       "71  gpt-4-0125-preview  MapReduce      128000  entertainment         707   \n",
       "72  gpt-4-0125-preview  MapReduce      128000  entertainment         636   \n",
       "73  gpt-4-0125-preview  MapReduce      128000       business         697   \n",
       "74  gpt-4-0125-preview  MapReduce      128000          sport         594   \n",
       "\n",
       "                                           transcript  \\\n",
       "0   Cuba winds back economic clock..Fidel Castro's...   \n",
       "1   Blair looks to election campaign..Tony Blair's...   \n",
       "2   New York rockers top talent poll..New York ele...   \n",
       "3   Terror suspects face house arrest..UK citizens...   \n",
       "4   'No more concessions' on terror..Charles Clark...   \n",
       "..                                                ...   \n",
       "70  Paris promise raises Welsh hopes..Has there be...   \n",
       "71  Rapper Kanye West's shrewd soul..US hip-hop st...   \n",
       "72  Redford's vision of Sundance..Despite sporting...   \n",
       "73  Water firm Suez in Argentina row..A conflict b...   \n",
       "74  Lions blow to World Cup winners..British and I...   \n",
       "\n",
       "                                     original summary  \\\n",
       "0   Fidel Castro's decision to ban all cash transa...   \n",
       "1   There was little in terms of concrete proposal...   \n",
       "2   New York electro-rock group The Bravery have c...   \n",
       "3   British citizens are being included in the cha...   \n",
       "4   On Monday, MPs voted 272-219 in favour of the ...   \n",
       "..                                                ...   \n",
       "70  But since they threw off the shackles against ...   \n",
       "71  Leaving his Chicago art school after only one ...   \n",
       "72  Redford wanted Sundance to be a platform for i...   \n",
       "73  The government has rejected the 60% rise and w...   \n",
       "74  But Woodward added: \"The key thing that I want...   \n",
       "\n",
       "                                              summary  \\\n",
       "0   Cuba has imposed a 10% tax on conversions betw...   \n",
       "1   Tony Blair's recent speech marked the start of...   \n",
       "2   The Bravery, a New York electro-rock group, to...   \n",
       "3   The UK government is considering new measures ...   \n",
       "4   Charles Clarke is standing firm on his anti-te...   \n",
       "..                                                ...   \n",
       "70  The article highlights the rising expectations...   \n",
       "71  Kanye West, a key figure in hip-hop, leads the...   \n",
       "72  Robert Redford founded the Sundance Film Festi...   \n",
       "73  The Argentine government is in a dispute with ...   \n",
       "74  Clive Woodward, coach of the British and Irish...   \n",
       "\n",
       "                                              grammar  \\\n",
       "0                                                  []   \n",
       "1   [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...   \n",
       "2   [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...   \n",
       "3                                                  []   \n",
       "4   [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...   \n",
       "..                                                ...   \n",
       "70                                                 []   \n",
       "71  [Offset 557, length 4, Rule ID: MORFOLOGIK_RUL...   \n",
       "72  [Offset 27, length 8, Rule ID: MORFOLOGIK_RULE...   \n",
       "73  [Offset 46, length 5, Rule ID: MORFOLOGIK_RULE...   \n",
       "74  [Offset 250, length 9, Rule ID: MORFOLOGIK_RUL...   \n",
       "\n",
       "                                     readability  \\\n",
       "0                            100 words required.   \n",
       "1                            100 words required.   \n",
       "2                            100 words required.   \n",
       "3                            100 words required.   \n",
       "4                            100 words required.   \n",
       "..                                           ...   \n",
       "70  score: 14.651290322580646, grade_level: '15'   \n",
       "71   score: 13.59773553719008, grade_level: '14'   \n",
       "72               score: 15.02, grade_level: '15'   \n",
       "73  score: 17.724833333333333, grade_level: '18'   \n",
       "74  score: 15.618651162790702, grade_level: '16'   \n",
       "\n",
       "                                                rouge  \\\n",
       "0   [{'rouge-1': {'r': 0.17006802721088435, 'p': 0...   \n",
       "1   [{'rouge-1': {'r': 0.17365269461077845, 'p': 0...   \n",
       "2   [{'rouge-1': {'r': 0.26666666666666666, 'p': 0...   \n",
       "3   [{'rouge-1': {'r': 0.16265060240963855, 'p': 0...   \n",
       "4   [{'rouge-1': {'r': 0.2751677852348993, 'p': 0....   \n",
       "..                                                ...   \n",
       "70  [{'rouge-1': {'r': 0.17391304347826086, 'p': 0...   \n",
       "71  [{'rouge-1': {'r': 0.2430939226519337, 'p': 0....   \n",
       "72  [{'rouge-1': {'r': 0.16129032258064516, 'p': 0...   \n",
       "73  [{'rouge-1': {'r': 0.2929936305732484, 'p': 0....   \n",
       "74  [{'rouge-1': {'r': 0.3105590062111801, 'p': 0....   \n",
       "\n",
       "                                           bert_score  time_taken  \\\n",
       "0   (tensor([0.9130]), tensor([0.8402]), tensor([0...    3.803998   \n",
       "1   (tensor([0.8809]), tensor([0.8259]), tensor([0...    4.186849   \n",
       "2   (tensor([0.9043]), tensor([0.8542]), tensor([0...    2.662916   \n",
       "3   (tensor([0.8905]), tensor([0.8307]), tensor([0...    3.348620   \n",
       "4   (tensor([0.8944]), tensor([0.8478]), tensor([0...    2.810315   \n",
       "..                                                ...         ...   \n",
       "70  ([tensor(0.8637)], [tensor(0.8236)], [tensor(0...   33.090430   \n",
       "71  ([tensor(0.8815)], [tensor(0.8397)], [tensor(0...   22.018759   \n",
       "72  ([tensor(0.8860)], [tensor(0.8411)], [tensor(0...   25.070732   \n",
       "73  ([tensor(0.8820)], [tensor(0.8585)], [tensor(0...   43.071345   \n",
       "74  ([tensor(0.8841)], [tensor(0.8443)], [tensor(0...   30.502107   \n",
       "\n",
       "                                              prompt  temperature      bleu  \n",
       "0   Write a concise summary of the following: {text}            0  0.016126  \n",
       "1   Write a concise summary of the following: {text}            0  0.016755  \n",
       "2   Write a concise summary of the following: {text}            0  0.057747  \n",
       "3   Write a concise summary of the following: {text}            0  0.005401  \n",
       "4   Write a concise summary of the following: {text}            0  0.061109  \n",
       "..                                               ...          ...       ...  \n",
       "70  Write a concise summary of the following: {text}            0  0.035691  \n",
       "71  Write a concise summary of the following: {text}            0  0.034853  \n",
       "72  Write a concise summary of the following: {text}            0  0.025239  \n",
       "73  Write a concise summary of the following: {text}            0  0.064537  \n",
       "74  Write a concise summary of the following: {text}            0  0.031168  \n",
       "\n",
       "[75 rows x 16 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scores.to_excel(\"./result/closed_source_model_topics_comparison.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Topic Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>method</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>topic</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>transcript</th>\n",
       "      <th>original summary</th>\n",
       "      <th>summary</th>\n",
       "      <th>grammar</th>\n",
       "      <th>readability</th>\n",
       "      <th>rouge</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>prompt</th>\n",
       "      <th>temperature</th>\n",
       "      <th>bleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, method, max_tokens, topic, num_tokens, transcript, original summary, summary, grammar, readability, rouge, bert_score, time_taken, prompt, temperature, bleu]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores = pd.read_excel(\"./result/closed_source_model_topics_comparison.xlsx\")\n",
    "print(df_scores.shape)\n",
    "df_scores.head(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>max_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text-bison-001</td>\n",
       "      <td>8196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           models  max_tokens\n",
       "0  text-bison-001        8196"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Models\n",
    "gcp_models = {\n",
    "\n",
    "    # Models selected\n",
    "    \"models\": [\n",
    "        # \"gemini-pro\",\n",
    "        # \"chat-bison-001\",\n",
    "        \"text-bison-001\"\n",
    "    ],\n",
    "\n",
    "    # Input Token\n",
    "    \"max_tokens\": [\n",
    "        # 30720,\n",
    "        # 4096,\n",
    "        8196,\n",
    "\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "df_gcp_models = pd.DataFrame(gcp_models)\n",
    "df_gcp_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text-bison-001\n",
      "Number of tokens: 778\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 95.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.08 seconds, 0.48 sentences/sec\n",
      "Number of tokens: 701\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 90.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.45 seconds, 0.41 sentences/sec\n",
      "Number of tokens: 728\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.60 seconds, 0.62 sentences/sec\n",
      "Number of tokens: 672\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 181.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.99 seconds, 0.50 sentences/sec\n",
      "Number of tokens: 654\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 125.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.15 seconds, 0.47 sentences/sec\n",
      "Number of tokens: 629\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 166.67it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.52 seconds, 0.66 sentences/sec\n",
      "Number of tokens: 704\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.08 seconds, 0.48 sentences/sec\n",
      "Number of tokens: 767\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 499.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.11 seconds, 0.47 sentences/sec\n",
      "Number of tokens: 879\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 122.94it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.30 seconds, 0.43 sentences/sec\n",
      "Number of tokens: 773\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 995.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.28 seconds, 0.44 sentences/sec\n",
      "Number of tokens: 699\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 111.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.06 seconds, 0.49 sentences/sec\n",
      "Number of tokens: 859\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 121.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.29 seconds, 0.44 sentences/sec\n",
      "Number of tokens: 828\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 153.46it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.50 seconds, 0.40 sentences/sec\n",
      "Number of tokens: 634\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 124.99it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.71 seconds, 0.58 sentences/sec\n",
      "Number of tokens: 801\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 499.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.40 seconds, 0.42 sentences/sec\n",
      "Number of tokens: 809\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 124.95it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.09 seconds, 0.48 sentences/sec\n",
      "Number of tokens: 854\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.56 seconds, 0.39 sentences/sec\n",
      "Number of tokens: 696\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 100.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.11 seconds, 0.47 sentences/sec\n",
      "Number of tokens: 701\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.10 seconds, 0.48 sentences/sec\n",
      "Number of tokens: 751\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 117.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.26 seconds, 0.44 sentences/sec\n",
      "Number of tokens: 815\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 32.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.08 seconds, 0.48 sentences/sec\n",
      "Number of tokens: 744\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.29 seconds, 0.44 sentences/sec\n",
      "Number of tokens: 639\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 111.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.01 seconds, 0.50 sentences/sec\n",
      "Number of tokens: 690\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 95.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.02 seconds, 0.50 sentences/sec\n",
      "Number of tokens: 600\n",
      "8196\n",
      "Number of chunks: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 97.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.12 seconds, 0.47 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "for model_index, model_row in df_gcp_models.iterrows():\n",
    "    model_name = model_row[\"models\"]\n",
    "    print(model_name)\n",
    "\n",
    "    temperature = 0\n",
    "\n",
    "    if \"chat\" in model_name.lower():\n",
    "      llm = ChatVertexAI(temperature=temperature, model=model_name, verbose=True)\n",
    "    else:\n",
    "      llm = VertexAI(temperature=temperature, model=model_name, verbose=True)\n",
    "    \n",
    "    prompt_template = \"\"\"Write a concise summary of the following: {text}\"\"\"\n",
    "    PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "    for index, row in bbc_train_df.iterrows():\n",
    "        method = \"MapReduce\"\n",
    "\n",
    "        # get the summary\n",
    "        start_time = time.time()\n",
    "        num_tokens = llm.get_num_tokens(row['transcript'])\n",
    "        print(\"Number of tokens:\", num_tokens)\n",
    "\n",
    "        max_tokens = model_row[\"max_tokens\"]\n",
    "        print(max_tokens)\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=max_tokens-100, chunk_overlap=100)\n",
    "        docs = text_splitter.create_documents([row['transcript']])\n",
    "        print(\"Number of chunks:\", len(docs))\n",
    "\n",
    "        summary_chain = load_summarize_chain(llm=llm, chain_type='map_reduce', token_max=max_tokens , map_prompt=PROMPT)\n",
    "        summary = summary_chain.run(docs)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        metrics = SummarizationMetrics(row['summary'], summary)\n",
    "\n",
    "        new_result = {\n",
    "            'model': model_name,\n",
    "            'method': method,\n",
    "            'max_tokens': max_tokens,\n",
    "            'topic': row[\"File_path\"],\n",
    "            'transcript': row['transcript'],\n",
    "            'original summary': row['summary'],\n",
    "            'summary': summary,\n",
    "            'rouge': metrics.rouge_scores(),\n",
    "            'bert_score': metrics.bert_score(),\n",
    "            'bleu': metrics.bleu_score(),\n",
    "            'time_taken': elapsed_time,\n",
    "            'grammar': metrics.grammar_check(),\n",
    "            'readability': metrics.readability_index(),\n",
    "            'num_tokens': num_tokens,\n",
    "            'prompt': prompt_template,\n",
    "            'temperature': temperature\n",
    "        }\n",
    "\n",
    "\n",
    "        new_row = pd.DataFrame([new_result])\n",
    "\n",
    "        df_scores = pd.concat([df_scores, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_excel(\"./result/closed_source_model_topics_comparison.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
