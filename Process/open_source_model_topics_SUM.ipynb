{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Source Modelling with BBC News Dataset\n",
    "\n",
    "For comparison of Summary in different aspects/topics.\n",
    "\n",
    "Understand LLM Strengths and Weaknesses:\n",
    "\n",
    "- **Identify domain-specific strengths**: Different LLMs are trained on different datasets and may excel in different domains. Comparing summaries across topics can help you identify which LLM performs best in a specific area relevant to your needs.\n",
    "\n",
    "- **Uncover biases and limitations**: LLMs can inherit biases from their training data. Comparing summaries can help you identify potential biases and limitations in different models, allowing you to choose the one with the least bias for your task.\n",
    "\n",
    "- **Evaluate factual accuracy**: Some LLMs prioritize fluency over factual accuracy, while others excel at fact-checking. Comparing summaries can help you assess the factual accuracy of each LLM and choose the one that best suits your need for reliable information.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. **Install & Import Necessary Libraries**\n",
    "\n",
    "2. **Helper Function**\n",
    "\n",
    "3. **Load Dataset (BBC News)** : Pick 5 rows of data per aspects\n",
    "\n",
    "4. **Top 5 Open Source Model Summarizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install & Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from helper.SummarizationMetrics import SummarizationMetrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import ast\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from  langchain import LLMChain, HuggingFacePipeline, PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from scipy.signal import argrelextrema\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_pipeline(model, tokenizer, chain_type, max_length, prompt=False):\n",
    "  pipeline = transformers.pipeline(\n",
    "      \"summarization\",\n",
    "      model=model,\n",
    "      tokenizer=tokenizer,\n",
    "      torch_dtype=torch.bfloat16,\n",
    "      trust_remote_code=True,\n",
    "      device_map=\"auto\",\n",
    "      max_length=max_length,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "  )\n",
    "  llm = HuggingFacePipeline(pipeline = pipeline)\n",
    "\n",
    "  if chain_type == \"map_reduce\":\n",
    "    if prompt:\n",
    "      prompt_template = \"\"\"Summarize this: ```{text}```\"\"\"\n",
    "      prompt_message = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "      \n",
    "      summary_chain = load_summarize_chain(llm=llm, chain_type=chain_type, token_max=max_length, prompt=prompt_message)\n",
    "    else:\n",
    "      summary_chain = load_summarize_chain(llm=llm, chain_type=chain_type, token_max=max_length)\n",
    "  else:\n",
    "    # can't get it to work with refine and stuff, think they updated the library but no documentation\n",
    "    # on how to set token_max\n",
    "    summary_chain = load_summarize_chain(llm=llm, chain_type=chain_type)\n",
    "  return summary_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBC DATASET\n",
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_path</th>\n",
       "      <th>Articles</th>\n",
       "      <th>Summaries</th>\n",
       "      <th>transcript</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Cuba winds back economic clock..Fidel Castro's...</td>\n",
       "      <td>Fidel Castro's decision to ban all cash transa...</td>\n",
       "      <td>Cuba winds back economic clock..Fidel Castro's...</td>\n",
       "      <td>Fidel Castro's decision to ban all cash transa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politics</td>\n",
       "      <td>Blair looks to election campaign..Tony Blair's...</td>\n",
       "      <td>There was little in terms of concrete proposal...</td>\n",
       "      <td>Blair looks to election campaign..Tony Blair's...</td>\n",
       "      <td>There was little in terms of concrete proposal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>New York rockers top talent poll..New York ele...</td>\n",
       "      <td>New York electro-rock group The Bravery have c...</td>\n",
       "      <td>New York rockers top talent poll..New York ele...</td>\n",
       "      <td>New York electro-rock group The Bravery have c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politics</td>\n",
       "      <td>Terror suspects face house arrest..UK citizens...</td>\n",
       "      <td>British citizens are being included in the cha...</td>\n",
       "      <td>Terror suspects face house arrest..UK citizens...</td>\n",
       "      <td>British citizens are being included in the cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>'No more concessions' on terror..Charles Clark...</td>\n",
       "      <td>On Monday, MPs voted 272-219 in favour of the ...</td>\n",
       "      <td>'No more concessions' on terror..Charles Clark...</td>\n",
       "      <td>On Monday, MPs voted 272-219 in favour of the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       File_path                                           Articles  \\\n",
       "0       business  Cuba winds back economic clock..Fidel Castro's...   \n",
       "1       politics  Blair looks to election campaign..Tony Blair's...   \n",
       "2  entertainment  New York rockers top talent poll..New York ele...   \n",
       "3       politics  Terror suspects face house arrest..UK citizens...   \n",
       "4       politics  'No more concessions' on terror..Charles Clark...   \n",
       "\n",
       "                                           Summaries  \\\n",
       "0  Fidel Castro's decision to ban all cash transa...   \n",
       "1  There was little in terms of concrete proposal...   \n",
       "2  New York electro-rock group The Bravery have c...   \n",
       "3  British citizens are being included in the cha...   \n",
       "4  On Monday, MPs voted 272-219 in favour of the ...   \n",
       "\n",
       "                                          transcript  \\\n",
       "0  Cuba winds back economic clock..Fidel Castro's...   \n",
       "1  Blair looks to election campaign..Tony Blair's...   \n",
       "2  New York rockers top talent poll..New York ele...   \n",
       "3  Terror suspects face house arrest..UK citizens...   \n",
       "4  'No more concessions' on terror..Charles Clark...   \n",
       "\n",
       "                                             summary  \n",
       "0  Fidel Castro's decision to ban all cash transa...  \n",
       "1  There was little in terms of concrete proposal...  \n",
       "2  New York electro-rock group The Bravery have c...  \n",
       "3  British citizens are being included in the cha...  \n",
       "4  On Monday, MPs voted 272-219 in favour of the ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_train_df = pd.read_excel(\"../Data/newsbbc_train.xlsx\")\n",
    "\n",
    "bbc_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get 5 records of data for each topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File_path\n",
       "business         5\n",
       "politics         5\n",
       "entertainment    5\n",
       "sport            5\n",
       "tech             5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly get 5 records from each topics.\n",
    "bbc_train_df = bbc_train_df.groupby('File_path').head(5)\n",
    "bbc_train_df['File_path'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_path</th>\n",
       "      <th>Articles</th>\n",
       "      <th>Summaries</th>\n",
       "      <th>transcript</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Cuba winds back economic clock..Fidel Castro's...</td>\n",
       "      <td>Fidel Castro's decision to ban all cash transa...</td>\n",
       "      <td>Cuba winds back economic clock..Fidel Castro's...</td>\n",
       "      <td>Fidel Castro's decision to ban all cash transa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politics</td>\n",
       "      <td>Blair looks to election campaign..Tony Blair's...</td>\n",
       "      <td>There was little in terms of concrete proposal...</td>\n",
       "      <td>Blair looks to election campaign..Tony Blair's...</td>\n",
       "      <td>There was little in terms of concrete proposal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>New York rockers top talent poll..New York ele...</td>\n",
       "      <td>New York electro-rock group The Bravery have c...</td>\n",
       "      <td>New York rockers top talent poll..New York ele...</td>\n",
       "      <td>New York electro-rock group The Bravery have c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politics</td>\n",
       "      <td>Terror suspects face house arrest..UK citizens...</td>\n",
       "      <td>British citizens are being included in the cha...</td>\n",
       "      <td>Terror suspects face house arrest..UK citizens...</td>\n",
       "      <td>British citizens are being included in the cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics</td>\n",
       "      <td>'No more concessions' on terror..Charles Clark...</td>\n",
       "      <td>On Monday, MPs voted 272-219 in favour of the ...</td>\n",
       "      <td>'No more concessions' on terror..Charles Clark...</td>\n",
       "      <td>On Monday, MPs voted 272-219 in favour of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>Howard denies split over ID cards..Michael How...</td>\n",
       "      <td>Michael Howard has denied his shadow cabinet w...</td>\n",
       "      <td>Howard denies split over ID cards..Michael How...</td>\n",
       "      <td>Michael Howard has denied his shadow cabinet w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sport</td>\n",
       "      <td>Chelsea denied by James heroics..A brave defen...</td>\n",
       "      <td>Chelsea were now looking more like Premiership...</td>\n",
       "      <td>Chelsea denied by James heroics..A brave defen...</td>\n",
       "      <td>Chelsea were now looking more like Premiership...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>politics</td>\n",
       "      <td>Guantanamo man 'suing government'..A British t...</td>\n",
       "      <td>He said he was sent there after being interrog...</td>\n",
       "      <td>Guantanamo man 'suing government'..A British t...</td>\n",
       "      <td>He said he was sent there after being interrog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>business</td>\n",
       "      <td>Could Yukos be a blessing in disguise?..Other ...</td>\n",
       "      <td>But it argues that more rigorous tax policing ...</td>\n",
       "      <td>Could Yukos be a blessing in disguise?..Other ...</td>\n",
       "      <td>But it argues that more rigorous tax policing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>business</td>\n",
       "      <td>Asian quake hits European shares..Shares in Eu...</td>\n",
       "      <td>The unfolding scale of the disaster in south A...</td>\n",
       "      <td>Asian quake hits European shares..Shares in Eu...</td>\n",
       "      <td>The unfolding scale of the disaster in south A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tech</td>\n",
       "      <td>The year search became personal..The odds are ...</td>\n",
       "      <td>Web users face a plethora of choices as each c...</td>\n",
       "      <td>The year search became personal..The odds are ...</td>\n",
       "      <td>Web users face a plethora of choices as each c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>tech</td>\n",
       "      <td>Doors open at biggest gadget fair..Thousands o...</td>\n",
       "      <td>He said the products which will be making wave...</td>\n",
       "      <td>Doors open at biggest gadget fair..Thousands o...</td>\n",
       "      <td>He said the products which will be making wave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tech</td>\n",
       "      <td>File-swappers ready new network..Legal attacks...</td>\n",
       "      <td>Sloncek said that currently only a Windows ver...</td>\n",
       "      <td>File-swappers ready new network..Legal attacks...</td>\n",
       "      <td>Sloncek said that currently only a Windows ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>tech</td>\n",
       "      <td>Toxic web links help virus spread..Virus write...</td>\n",
       "      <td>A Windows virus called Bofra is turning infect...</td>\n",
       "      <td>Toxic web links help virus spread..Virus write...</td>\n",
       "      <td>A Windows virus called Bofra is turning infect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>tech</td>\n",
       "      <td>Mobiles 'not media players yet'..Mobiles are n...</td>\n",
       "      <td>The service uses 3GP technology, one of the st...</td>\n",
       "      <td>Mobiles 'not media players yet'..Mobiles are n...</td>\n",
       "      <td>The service uses 3GP technology, one of the st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sport</td>\n",
       "      <td>European medal chances improve..What have the ...</td>\n",
       "      <td>Diane Allahgreen has been our best hurdler for...</td>\n",
       "      <td>European medal chances improve..What have the ...</td>\n",
       "      <td>Diane Allahgreen has been our best hurdler for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>business</td>\n",
       "      <td>Disaster claims 'less than $10bn'..Insurers ha...</td>\n",
       "      <td>The impact on US insurance companies is not ex...</td>\n",
       "      <td>Disaster claims 'less than $10bn'..Insurers ha...</td>\n",
       "      <td>The impact on US insurance companies is not ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Franz Ferdinand's art school lesson..Scottish ...</td>\n",
       "      <td>The buzz about the band soon spread around the...</td>\n",
       "      <td>Franz Ferdinand's art school lesson..Scottish ...</td>\n",
       "      <td>The buzz about the band soon spread around the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sport</td>\n",
       "      <td>Man Utd stroll to Cup win..Wayne Rooney made a...</td>\n",
       "      <td>But there was nothing Martyn could do when Uni...</td>\n",
       "      <td>Man Utd stroll to Cup win..Wayne Rooney made a...</td>\n",
       "      <td>But there was nothing Martyn could do when Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Stars shine on Bafta red carpet..Hollywood sta...</td>\n",
       "      <td>Keanu Reeves, who presented the best actress a...</td>\n",
       "      <td>Stars shine on Bafta red carpet..Hollywood sta...</td>\n",
       "      <td>Keanu Reeves, who presented the best actress a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>sport</td>\n",
       "      <td>Paris promise raises Welsh hopes..Has there be...</td>\n",
       "      <td>But since they threw off the shackles against ...</td>\n",
       "      <td>Paris promise raises Welsh hopes..Has there be...</td>\n",
       "      <td>But since they threw off the shackles against ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Rapper Kanye West's shrewd soul..US hip-hop st...</td>\n",
       "      <td>Leaving his Chicago art school after only one ...</td>\n",
       "      <td>Rapper Kanye West's shrewd soul..US hip-hop st...</td>\n",
       "      <td>Leaving his Chicago art school after only one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>Redford's vision of Sundance..Despite sporting...</td>\n",
       "      <td>Redford wanted Sundance to be a platform for i...</td>\n",
       "      <td>Redford's vision of Sundance..Despite sporting...</td>\n",
       "      <td>Redford wanted Sundance to be a platform for i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>business</td>\n",
       "      <td>Water firm Suez in Argentina row..A conflict b...</td>\n",
       "      <td>The government has rejected the 60% rise and w...</td>\n",
       "      <td>Water firm Suez in Argentina row..A conflict b...</td>\n",
       "      <td>The government has rejected the 60% rise and w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>sport</td>\n",
       "      <td>Lions blow to World Cup winners..British and I...</td>\n",
       "      <td>But Woodward added: \"The key thing that I want...</td>\n",
       "      <td>Lions blow to World Cup winners..British and I...</td>\n",
       "      <td>But Woodward added: \"The key thing that I want...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        File_path                                           Articles  \\\n",
       "0        business  Cuba winds back economic clock..Fidel Castro's...   \n",
       "1        politics  Blair looks to election campaign..Tony Blair's...   \n",
       "2   entertainment  New York rockers top talent poll..New York ele...   \n",
       "3        politics  Terror suspects face house arrest..UK citizens...   \n",
       "4        politics  'No more concessions' on terror..Charles Clark...   \n",
       "5        politics  Howard denies split over ID cards..Michael How...   \n",
       "6           sport  Chelsea denied by James heroics..A brave defen...   \n",
       "7        politics  Guantanamo man 'suing government'..A British t...   \n",
       "13       business  Could Yukos be a blessing in disguise?..Other ...   \n",
       "14       business  Asian quake hits European shares..Shares in Eu...   \n",
       "15           tech  The year search became personal..The odds are ...   \n",
       "17           tech  Doors open at biggest gadget fair..Thousands o...   \n",
       "18           tech  File-swappers ready new network..Legal attacks...   \n",
       "19           tech  Toxic web links help virus spread..Virus write...   \n",
       "20           tech  Mobiles 'not media players yet'..Mobiles are n...   \n",
       "24          sport  European medal chances improve..What have the ...   \n",
       "25       business  Disaster claims 'less than $10bn'..Insurers ha...   \n",
       "28  entertainment  Franz Ferdinand's art school lesson..Scottish ...   \n",
       "33          sport  Man Utd stroll to Cup win..Wayne Rooney made a...   \n",
       "37  entertainment  Stars shine on Bafta red carpet..Hollywood sta...   \n",
       "40          sport  Paris promise raises Welsh hopes..Has there be...   \n",
       "46  entertainment  Rapper Kanye West's shrewd soul..US hip-hop st...   \n",
       "47  entertainment  Redford's vision of Sundance..Despite sporting...   \n",
       "50       business  Water firm Suez in Argentina row..A conflict b...   \n",
       "75          sport  Lions blow to World Cup winners..British and I...   \n",
       "\n",
       "                                            Summaries  \\\n",
       "0   Fidel Castro's decision to ban all cash transa...   \n",
       "1   There was little in terms of concrete proposal...   \n",
       "2   New York electro-rock group The Bravery have c...   \n",
       "3   British citizens are being included in the cha...   \n",
       "4   On Monday, MPs voted 272-219 in favour of the ...   \n",
       "5   Michael Howard has denied his shadow cabinet w...   \n",
       "6   Chelsea were now looking more like Premiership...   \n",
       "7   He said he was sent there after being interrog...   \n",
       "13  But it argues that more rigorous tax policing ...   \n",
       "14  The unfolding scale of the disaster in south A...   \n",
       "15  Web users face a plethora of choices as each c...   \n",
       "17  He said the products which will be making wave...   \n",
       "18  Sloncek said that currently only a Windows ver...   \n",
       "19  A Windows virus called Bofra is turning infect...   \n",
       "20  The service uses 3GP technology, one of the st...   \n",
       "24  Diane Allahgreen has been our best hurdler for...   \n",
       "25  The impact on US insurance companies is not ex...   \n",
       "28  The buzz about the band soon spread around the...   \n",
       "33  But there was nothing Martyn could do when Uni...   \n",
       "37  Keanu Reeves, who presented the best actress a...   \n",
       "40  But since they threw off the shackles against ...   \n",
       "46  Leaving his Chicago art school after only one ...   \n",
       "47  Redford wanted Sundance to be a platform for i...   \n",
       "50  The government has rejected the 60% rise and w...   \n",
       "75  But Woodward added: \"The key thing that I want...   \n",
       "\n",
       "                                           transcript  \\\n",
       "0   Cuba winds back economic clock..Fidel Castro's...   \n",
       "1   Blair looks to election campaign..Tony Blair's...   \n",
       "2   New York rockers top talent poll..New York ele...   \n",
       "3   Terror suspects face house arrest..UK citizens...   \n",
       "4   'No more concessions' on terror..Charles Clark...   \n",
       "5   Howard denies split over ID cards..Michael How...   \n",
       "6   Chelsea denied by James heroics..A brave defen...   \n",
       "7   Guantanamo man 'suing government'..A British t...   \n",
       "13  Could Yukos be a blessing in disguise?..Other ...   \n",
       "14  Asian quake hits European shares..Shares in Eu...   \n",
       "15  The year search became personal..The odds are ...   \n",
       "17  Doors open at biggest gadget fair..Thousands o...   \n",
       "18  File-swappers ready new network..Legal attacks...   \n",
       "19  Toxic web links help virus spread..Virus write...   \n",
       "20  Mobiles 'not media players yet'..Mobiles are n...   \n",
       "24  European medal chances improve..What have the ...   \n",
       "25  Disaster claims 'less than $10bn'..Insurers ha...   \n",
       "28  Franz Ferdinand's art school lesson..Scottish ...   \n",
       "33  Man Utd stroll to Cup win..Wayne Rooney made a...   \n",
       "37  Stars shine on Bafta red carpet..Hollywood sta...   \n",
       "40  Paris promise raises Welsh hopes..Has there be...   \n",
       "46  Rapper Kanye West's shrewd soul..US hip-hop st...   \n",
       "47  Redford's vision of Sundance..Despite sporting...   \n",
       "50  Water firm Suez in Argentina row..A conflict b...   \n",
       "75  Lions blow to World Cup winners..British and I...   \n",
       "\n",
       "                                              summary  \n",
       "0   Fidel Castro's decision to ban all cash transa...  \n",
       "1   There was little in terms of concrete proposal...  \n",
       "2   New York electro-rock group The Bravery have c...  \n",
       "3   British citizens are being included in the cha...  \n",
       "4   On Monday, MPs voted 272-219 in favour of the ...  \n",
       "5   Michael Howard has denied his shadow cabinet w...  \n",
       "6   Chelsea were now looking more like Premiership...  \n",
       "7   He said he was sent there after being interrog...  \n",
       "13  But it argues that more rigorous tax policing ...  \n",
       "14  The unfolding scale of the disaster in south A...  \n",
       "15  Web users face a plethora of choices as each c...  \n",
       "17  He said the products which will be making wave...  \n",
       "18  Sloncek said that currently only a Windows ver...  \n",
       "19  A Windows virus called Bofra is turning infect...  \n",
       "20  The service uses 3GP technology, one of the st...  \n",
       "24  Diane Allahgreen has been our best hurdler for...  \n",
       "25  The impact on US insurance companies is not ex...  \n",
       "28  The buzz about the band soon spread around the...  \n",
       "33  But there was nothing Martyn could do when Uni...  \n",
       "37  Keanu Reeves, who presented the best actress a...  \n",
       "40  But since they threw off the shackles against ...  \n",
       "46  Leaving his Chicago art school after only one ...  \n",
       "47  Redford wanted Sundance to be a platform for i...  \n",
       "50  The government has rejected the 60% rise and w...  \n",
       "75  But Woodward added: \"The key thing that I want...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running too much models at once it could take very long, also Kernel will stop.\n",
    "\n",
    "Recommend: Run model separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>philschmid/bart-large-cnn-samsum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pszemraj/long-t5-tglobal-base-16384-book-summary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Models\n",
       "0                  philschmid/bart-large-cnn-samsum\n",
       "1  pszemraj/long-t5-tglobal-base-16384-book-summary"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_scores = pd.read_excel(\"../Process/result/open_source_model_topics_comparison.xlsx\")\n",
    "\n",
    "# Defined the top 5 Models\n",
    "data = {\n",
    "    'Models': [\n",
    "        # 'pszemraj/led-base-book-summary',\n",
    "        # 'pszemraj/led-large-book-summary',\n",
    "        # 'HHousen/distil-led-large-cnn-16384',\n",
    "        'philschmid/bart-large-cnn-samsum',\n",
    "        'pszemraj/long-t5-tglobal-base-16384-book-summary'\n",
    "    ]\n",
    "}\n",
    "\n",
    "top_5_models = pd.DataFrame(data)\n",
    "top_5_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>method</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>topic</th>\n",
       "      <th>transcript</th>\n",
       "      <th>original summary</th>\n",
       "      <th>summary</th>\n",
       "      <th>rouge</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bleu</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>grammar</th>\n",
       "      <th>readability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [model, method, max_tokens, topic, transcript, original summary, summary, rouge, bert_score, bleu, time_taken, grammar, readability]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame to store summary result and performance.\n",
    "df_scores = pd.DataFrame(columns=['model', 'method', 'max_tokens', \"topic\" ,'transcript','original summary', 'summary', 'rouge','bert_score', 'bleu', 'time_taken', 'grammar', 'readability'])\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>method</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>topic</th>\n",
       "      <th>transcript</th>\n",
       "      <th>original summary</th>\n",
       "      <th>summary</th>\n",
       "      <th>rouge</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bleu</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>grammar</th>\n",
       "      <th>readability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pszemraj/led-base-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16384</td>\n",
       "      <td>business</td>\n",
       "      <td>Cuba winds back economic clock..Fidel Castro's...</td>\n",
       "      <td>Fidel Castro's decision to ban all cash transa...</td>\n",
       "      <td>The following is a summary of the announcement...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.38095238095238093, 'p': 0...</td>\n",
       "      <td>(tensor([0.8857]), tensor([0.8723]), tensor([0...</td>\n",
       "      <td>0.218163</td>\n",
       "      <td>19.899436</td>\n",
       "      <td>[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...</td>\n",
       "      <td>score: 13.012869198312234, grade_level: '13'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pszemraj/led-base-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16384</td>\n",
       "      <td>politics</td>\n",
       "      <td>Blair looks to election campaign..Tony Blair's...</td>\n",
       "      <td>There was little in terms of concrete proposal...</td>\n",
       "      <td>In this brief summary, we summarize the speech...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.2874251497005988, 'p': 0....</td>\n",
       "      <td>(tensor([0.8664]), tensor([0.8317]), tensor([0...</td>\n",
       "      <td>0.095036</td>\n",
       "      <td>21.163034</td>\n",
       "      <td>[Match({'ruleId': 'EN_COMPOUNDS', 'message': '...</td>\n",
       "      <td>score: 9.665454545454548, grade_level: '10'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pszemraj/led-base-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16384</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>New York rockers top talent poll..New York ele...</td>\n",
       "      <td>New York electro-rock group The Bravery have c...</td>\n",
       "      <td>The narrator of this piece originally appeared...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.4148148148148148, 'p': 0....</td>\n",
       "      <td>(tensor([0.8524]), tensor([0.8609]), tensor([0...</td>\n",
       "      <td>0.175990</td>\n",
       "      <td>28.523080</td>\n",
       "      <td>[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...</td>\n",
       "      <td>score: 11.396671501087742, grade_level: '11'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pszemraj/led-base-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16384</td>\n",
       "      <td>politics</td>\n",
       "      <td>Terror suspects face house arrest..UK citizens...</td>\n",
       "      <td>British citizens are being included in the cha...</td>\n",
       "      <td>The Home Secretary, Charles Clarke, has outlin...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.2710843373493976, 'p': 0....</td>\n",
       "      <td>(tensor([0.8730]), tensor([0.8468]), tensor([0...</td>\n",
       "      <td>0.056949</td>\n",
       "      <td>24.956480</td>\n",
       "      <td>[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...</td>\n",
       "      <td>score: 11.617224157955867, grade_level: '12'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pszemraj/led-base-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16384</td>\n",
       "      <td>politics</td>\n",
       "      <td>'No more concessions' on terror..Charles Clark...</td>\n",
       "      <td>On Monday, MPs voted 272-219 in favour of the ...</td>\n",
       "      <td>The English Parliament passes the Prevention o...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.3959731543624161, 'p': 0....</td>\n",
       "      <td>(tensor([0.8890]), tensor([0.8720]), tensor([0...</td>\n",
       "      <td>0.167719</td>\n",
       "      <td>21.425882</td>\n",
       "      <td>[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...</td>\n",
       "      <td>score: 9.484285714285715, grade_level: '9'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model     method  max_tokens          topic  \\\n",
       "0  pszemraj/led-base-book-summary  MapReduce       16384       business   \n",
       "1  pszemraj/led-base-book-summary  MapReduce       16384       politics   \n",
       "2  pszemraj/led-base-book-summary  MapReduce       16384  entertainment   \n",
       "3  pszemraj/led-base-book-summary  MapReduce       16384       politics   \n",
       "4  pszemraj/led-base-book-summary  MapReduce       16384       politics   \n",
       "\n",
       "                                          transcript  \\\n",
       "0  Cuba winds back economic clock..Fidel Castro's...   \n",
       "1  Blair looks to election campaign..Tony Blair's...   \n",
       "2  New York rockers top talent poll..New York ele...   \n",
       "3  Terror suspects face house arrest..UK citizens...   \n",
       "4  'No more concessions' on terror..Charles Clark...   \n",
       "\n",
       "                                    original summary  \\\n",
       "0  Fidel Castro's decision to ban all cash transa...   \n",
       "1  There was little in terms of concrete proposal...   \n",
       "2  New York electro-rock group The Bravery have c...   \n",
       "3  British citizens are being included in the cha...   \n",
       "4  On Monday, MPs voted 272-219 in favour of the ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The following is a summary of the announcement...   \n",
       "1  In this brief summary, we summarize the speech...   \n",
       "2  The narrator of this piece originally appeared...   \n",
       "3  The Home Secretary, Charles Clarke, has outlin...   \n",
       "4  The English Parliament passes the Prevention o...   \n",
       "\n",
       "                                               rouge  \\\n",
       "0  [{'rouge-1': {'r': 0.38095238095238093, 'p': 0...   \n",
       "1  [{'rouge-1': {'r': 0.2874251497005988, 'p': 0....   \n",
       "2  [{'rouge-1': {'r': 0.4148148148148148, 'p': 0....   \n",
       "3  [{'rouge-1': {'r': 0.2710843373493976, 'p': 0....   \n",
       "4  [{'rouge-1': {'r': 0.3959731543624161, 'p': 0....   \n",
       "\n",
       "                                          bert_score      bleu  time_taken  \\\n",
       "0  (tensor([0.8857]), tensor([0.8723]), tensor([0...  0.218163   19.899436   \n",
       "1  (tensor([0.8664]), tensor([0.8317]), tensor([0...  0.095036   21.163034   \n",
       "2  (tensor([0.8524]), tensor([0.8609]), tensor([0...  0.175990   28.523080   \n",
       "3  (tensor([0.8730]), tensor([0.8468]), tensor([0...  0.056949   24.956480   \n",
       "4  (tensor([0.8890]), tensor([0.8720]), tensor([0...  0.167719   21.425882   \n",
       "\n",
       "                                             grammar  \\\n",
       "0  [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...   \n",
       "1  [Match({'ruleId': 'EN_COMPOUNDS', 'message': '...   \n",
       "2  [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...   \n",
       "3  [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...   \n",
       "4  [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...   \n",
       "\n",
       "                                    readability  \n",
       "0  score: 13.012869198312234, grade_level: '13'  \n",
       "1   score: 9.665454545454548, grade_level: '10'  \n",
       "2  score: 11.396671501087742, grade_level: '11'  \n",
       "3  score: 11.617224157955867, grade_level: '12'  \n",
       "4    score: 9.484285714285715, grade_level: '9'  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file, if want to continue add on summary result.\n",
    "df_scores = pd.read_excel(\"../Process/result/open_source_model_topics_comparison.xlsx\")\n",
    "print(df_scores.shape)\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 Open Source Model Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "philschmid/bart-large-cnn-samsum\n",
      "1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 785. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=392)\n",
      "Your max_length is set to 1024, but your input_length is only 110. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.08 seconds, 0.48 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 716. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=358)\n",
      "Your max_length is set to 1024, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.10 seconds, 0.48 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 722. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=361)\n",
      "Your max_length is set to 1024, but your input_length is only 94. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 71.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.66 seconds, 0.60 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 688. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=344)\n",
      "Your max_length is set to 1024, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.35 seconds, 0.43 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 665. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=332)\n",
      "Your max_length is set to 1024, but your input_length is only 93. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.05 seconds, 0.49 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 657. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=328)\n",
      "Your max_length is set to 1024, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.82 seconds, 0.55 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 737. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=368)\n",
      "Your max_length is set to 1024, but your input_length is only 102. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 34.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.77 seconds, 0.26 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 757. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=378)\n",
      "Your max_length is set to 1024, but your input_length is only 90. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 58.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.10 seconds, 0.32 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 885. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=442)\n",
      "Your max_length is set to 1024, but your input_length is only 94. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 199.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.85 seconds, 0.35 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 763. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=381)\n",
      "Your max_length is set to 1024, but your input_length is only 115. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=57)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.27 seconds, 0.31 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 725. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=362)\n",
      "Your max_length is set to 1024, but your input_length is only 94. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=47)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 499.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.73 seconds, 0.37 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 847. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=423)\n",
      "Your max_length is set to 1024, but your input_length is only 110. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=55)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 500.75it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.51 seconds, 0.40 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 866. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=433)\n",
      "Your max_length is set to 1024, but your input_length is only 101. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.44 seconds, 0.29 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 668. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=334)\n",
      "Your max_length is set to 1024, but your input_length is only 93. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.62 seconds, 0.38 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 812. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=406)\n",
      "Your max_length is set to 1024, but your input_length is only 96. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 495.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.40 seconds, 0.42 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 820. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=410)\n",
      "Your max_length is set to 1024, but your input_length is only 81. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.78 seconds, 0.36 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 859. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=429)\n",
      "Your max_length is set to 1024, but your input_length is only 175. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=87)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.49 seconds, 0.29 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 713. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=356)\n",
      "Your max_length is set to 1024, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.35 seconds, 0.42 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 731. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=365)\n",
      "Your max_length is set to 1024, but your input_length is only 78. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 166.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.04 seconds, 0.33 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 803. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=401)\n",
      "Your max_length is set to 1024, but your input_length is only 78. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.76 seconds, 0.36 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 828. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=414)\n",
      "Your max_length is set to 1024, but your input_length is only 90. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 403.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.54 seconds, 0.39 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 741. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=370)\n",
      "Your max_length is set to 1024, but your input_length is only 103. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 500.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.46 seconds, 0.29 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 661. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=330)\n",
      "Your max_length is set to 1024, but your input_length is only 79. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 99.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.43 seconds, 0.41 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 710. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=355)\n",
      "Your max_length is set to 1024, but your input_length is only 129. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=64)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 100.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.79 seconds, 0.36 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 612. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=306)\n",
      "Your max_length is set to 1024, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 495.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.37 seconds, 0.42 sentences/sec\n",
      "pszemraj/long-t5-tglobal-base-16384-book-summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 872. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=436)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000000000000019884624838656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 183. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=91)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 500.22it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.39 seconds, 0.42 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 793. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=396)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 143. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=71)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 200.00it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.24 seconds, 0.45 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 828. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=414)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 107. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.04it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.66 seconds, 0.60 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 802. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=401)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 160. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=80)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.19 seconds, 0.31 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 760. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=380)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 102.75it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.62 seconds, 0.38 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 724. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=362)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 209. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=104)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 224.16it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.20 seconds, 0.46 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 895. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=447)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 258. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=129)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 308.63it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.70 seconds, 0.37 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 899. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=449)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 132. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=66)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 498.37it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.69 seconds, 0.37 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 981. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=490)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 145. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=72)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 165.92it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.95 seconds, 0.34 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 824. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=412)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 103. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=51)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.59it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.76 seconds, 0.36 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 792. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=396)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 139. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=69)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 314.75it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.10 seconds, 0.32 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 903. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=451)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 68. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=34)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 500.04it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.63 seconds, 0.38 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 955. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=477)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.12it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.87 seconds, 0.35 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 747. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=373)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 123. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.93it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.62 seconds, 0.38 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 860. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=430)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 138. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=69)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 287.38it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.72 seconds, 0.27 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 891. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=445)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 123. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.87it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.34 seconds, 0.23 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 930. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=465)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 64. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 250.00it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 4.51 seconds, 0.22 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 770. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=385)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 121. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=60)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 98.43it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.37 seconds, 0.30 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 880. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=440)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 185. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=92)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.34it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.30 seconds, 0.30 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 907. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=453)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 75. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.28it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.35 seconds, 0.30 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 910. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=455)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 92. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=46)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.20it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.30 seconds, 0.43 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 837. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=418)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 164. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=82)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 249.84it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.91 seconds, 0.26 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 739. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=369)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 107. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=53)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 142.90it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.78 seconds, 0.36 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 805. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=402)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 163. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=81)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 333.54it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 3.49 seconds, 0.29 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 679. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=339)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Your max_length is set to 1000000000000000019884624838656, but your input_length is only 73. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:896: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 166.78it/s]\n",
      "c:\\Users\\Zhang Xiang\\Desktop\\Year 3\\Sem 2\\FYPJ\\Project\\RD_TextSummarizerForWebinar\\Development\\.venv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.61 seconds, 0.38 sentences/sec\n"
     ]
    }
   ],
   "source": [
    "for model_name in top_5_models['Models']:\n",
    "    print(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    print(tokenizer.model_max_length)\n",
    "    for index, row in bbc_train_df.iterrows():\n",
    "        method = \"MapReduce\"\n",
    "        \n",
    "        # get the summary\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "        max_tokens = tokenizer.model_max_length\n",
    "        summary_chain = summ_pipeline(model, tokenizer, \"map_reduce\", max_tokens)\n",
    "\n",
    "        # Used for efficient tokenization and processing of long texts when working with language models\n",
    "        text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=max_tokens-100, chunk_overlap=100)\n",
    "        docs = text_splitter.create_documents([row[\"transcript\"]])\n",
    "        summary = summary_chain.run(docs)\n",
    "\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        metrics = SummarizationMetrics(row['summary'], summary)\n",
    "\n",
    "        new_result = {\n",
    "            'model': model_name,\n",
    "            'method': method,\n",
    "            'max_tokens': max_tokens,\n",
    "            'topic': row[\"File_path\"],\n",
    "            'transcript': row['transcript'],\n",
    "            'original summary': row['summary'],\n",
    "            'summary': summary,\n",
    "            'rouge': metrics.rouge_scores(),\n",
    "            'bert_score': metrics.bert_score(),\n",
    "            'bleu': metrics.bleu_score(),\n",
    "            'time_taken': elapsed_time,\n",
    "            'grammar': metrics.grammar_check(),\n",
    "            'readability': metrics.readability_index()\n",
    "        }\n",
    "\n",
    "\n",
    "        new_row = pd.DataFrame([new_result])\n",
    "\n",
    "        df_scores = pd.concat([df_scores, new_row], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>method</th>\n",
       "      <th>max_tokens</th>\n",
       "      <th>topic</th>\n",
       "      <th>transcript</th>\n",
       "      <th>original summary</th>\n",
       "      <th>summary</th>\n",
       "      <th>rouge</th>\n",
       "      <th>bert_score</th>\n",
       "      <th>bleu</th>\n",
       "      <th>time_taken</th>\n",
       "      <th>grammar</th>\n",
       "      <th>readability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pszemraj/led-base-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16384</td>\n",
       "      <td>business</td>\n",
       "      <td>Cuba winds back economic clock..Fidel Castro's...</td>\n",
       "      <td>Fidel Castro's decision to ban all cash transa...</td>\n",
       "      <td>The following is a summary of the announcement...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.38095238095238093, 'p': 0...</td>\n",
       "      <td>(tensor([0.8857]), tensor([0.8723]), tensor([0...</td>\n",
       "      <td>2.181628e-01</td>\n",
       "      <td>19.899436</td>\n",
       "      <td>[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...</td>\n",
       "      <td>score: 13.012869198312234, grade_level: '13'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pszemraj/led-base-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16384</td>\n",
       "      <td>politics</td>\n",
       "      <td>Blair looks to election campaign..Tony Blair's...</td>\n",
       "      <td>There was little in terms of concrete proposal...</td>\n",
       "      <td>In this brief summary, we summarize the speech...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.2874251497005988, 'p': 0....</td>\n",
       "      <td>(tensor([0.8664]), tensor([0.8317]), tensor([0...</td>\n",
       "      <td>9.503589e-02</td>\n",
       "      <td>21.163034</td>\n",
       "      <td>[Match({'ruleId': 'EN_COMPOUNDS', 'message': '...</td>\n",
       "      <td>score: 9.665454545454548, grade_level: '10'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pszemraj/led-base-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16384</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>New York rockers top talent poll..New York ele...</td>\n",
       "      <td>New York electro-rock group The Bravery have c...</td>\n",
       "      <td>The narrator of this piece originally appeared...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.4148148148148148, 'p': 0....</td>\n",
       "      <td>(tensor([0.8524]), tensor([0.8609]), tensor([0...</td>\n",
       "      <td>1.759903e-01</td>\n",
       "      <td>28.523080</td>\n",
       "      <td>[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...</td>\n",
       "      <td>score: 11.396671501087742, grade_level: '11'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pszemraj/led-base-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16384</td>\n",
       "      <td>politics</td>\n",
       "      <td>Terror suspects face house arrest..UK citizens...</td>\n",
       "      <td>British citizens are being included in the cha...</td>\n",
       "      <td>The Home Secretary, Charles Clarke, has outlin...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.2710843373493976, 'p': 0....</td>\n",
       "      <td>(tensor([0.8730]), tensor([0.8468]), tensor([0...</td>\n",
       "      <td>5.694877e-02</td>\n",
       "      <td>24.956480</td>\n",
       "      <td>[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...</td>\n",
       "      <td>score: 11.617224157955867, grade_level: '12'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pszemraj/led-base-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>16384</td>\n",
       "      <td>politics</td>\n",
       "      <td>'No more concessions' on terror..Charles Clark...</td>\n",
       "      <td>On Monday, MPs voted 272-219 in favour of the ...</td>\n",
       "      <td>The English Parliament passes the Prevention o...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.3959731543624161, 'p': 0....</td>\n",
       "      <td>(tensor([0.8890]), tensor([0.8720]), tensor([0...</td>\n",
       "      <td>1.677187e-01</td>\n",
       "      <td>21.425882</td>\n",
       "      <td>[Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...</td>\n",
       "      <td>score: 9.484285714285715, grade_level: '9'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>pszemraj/long-t5-tglobal-base-16384-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>sport</td>\n",
       "      <td>Paris promise raises Welsh hopes..Has there be...</td>\n",
       "      <td>But since they threw off the shackles against ...</td>\n",
       "      <td>This brief paper gives an update on the progre...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.08695652173913043, 'p': 0...</td>\n",
       "      <td>([tensor(0.8620)], [tensor(0.8025)], [tensor(0...</td>\n",
       "      <td>4.201394e-157</td>\n",
       "      <td>9.210987</td>\n",
       "      <td>[]</td>\n",
       "      <td>100 words required.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>pszemraj/long-t5-tglobal-base-16384-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Rapper Kanye West's shrewd soul..US hip-hop st...</td>\n",
       "      <td>Leaving his Chicago art school after only one ...</td>\n",
       "      <td>The Narrator informs the audience that Kany We...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.09392265193370165, 'p': 0...</td>\n",
       "      <td>([tensor(0.8522)], [tensor(0.7918)], [tensor(0...</td>\n",
       "      <td>1.260210e-156</td>\n",
       "      <td>15.023796</td>\n",
       "      <td>[Offset 39, length 4, Rule ID: MORFOLOGIK_RULE...</td>\n",
       "      <td>100 words required.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>pszemraj/long-t5-tglobal-base-16384-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Redford's vision of Sundance..Despite sporting...</td>\n",
       "      <td>Redford wanted Sundance to be a platform for i...</td>\n",
       "      <td>Robert Redford is the founder and president of...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.07096774193548387, 'p': 0...</td>\n",
       "      <td>([tensor(0.8724)], [tensor(0.8100)], [tensor(0...</td>\n",
       "      <td>8.507861e-159</td>\n",
       "      <td>8.595882</td>\n",
       "      <td>[Offset 51, length 8, Rule ID: MORFOLOGIK_RULE...</td>\n",
       "      <td>100 words required.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>pszemraj/long-t5-tglobal-base-16384-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>business</td>\n",
       "      <td>Water firm Suez in Argentina row..A conflict b...</td>\n",
       "      <td>The government has rejected the 60% rise and w...</td>\n",
       "      <td>This paper describes a dispute between the Fre...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.07643312101910828, 'p': 0...</td>\n",
       "      <td>([tensor(0.8490)], [tensor(0.8013)], [tensor(0...</td>\n",
       "      <td>2.727474e-81</td>\n",
       "      <td>16.457350</td>\n",
       "      <td>[Offset 106, length 9, Rule ID: MORFOLOGIK_RUL...</td>\n",
       "      <td>100 words required.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>pszemraj/long-t5-tglobal-base-16384-book-summary</td>\n",
       "      <td>MapReduce</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>sport</td>\n",
       "      <td>Lions blow to World Cup winners..British and I...</td>\n",
       "      <td>But Woodward added: \"The key thing that I want...</td>\n",
       "      <td>The Lions coach is preparing for the Rugby Wor...</td>\n",
       "      <td>[{'rouge-1': {'r': 0.06832298136645963, 'p': 0...</td>\n",
       "      <td>([tensor(0.8783)], [tensor(0.8040)], [tensor(0...</td>\n",
       "      <td>2.898418e-83</td>\n",
       "      <td>8.036771</td>\n",
       "      <td>[Offset 67, length 4, Rule ID: COMMA_COMPOUND_...</td>\n",
       "      <td>100 words required.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model     method  \\\n",
       "0                      pszemraj/led-base-book-summary  MapReduce   \n",
       "1                      pszemraj/led-base-book-summary  MapReduce   \n",
       "2                      pszemraj/led-base-book-summary  MapReduce   \n",
       "3                      pszemraj/led-base-book-summary  MapReduce   \n",
       "4                      pszemraj/led-base-book-summary  MapReduce   \n",
       "..                                                ...        ...   \n",
       "120  pszemraj/long-t5-tglobal-base-16384-book-summary  MapReduce   \n",
       "121  pszemraj/long-t5-tglobal-base-16384-book-summary  MapReduce   \n",
       "122  pszemraj/long-t5-tglobal-base-16384-book-summary  MapReduce   \n",
       "123  pszemraj/long-t5-tglobal-base-16384-book-summary  MapReduce   \n",
       "124  pszemraj/long-t5-tglobal-base-16384-book-summary  MapReduce   \n",
       "\n",
       "                          max_tokens          topic  \\\n",
       "0                              16384       business   \n",
       "1                              16384       politics   \n",
       "2                              16384  entertainment   \n",
       "3                              16384       politics   \n",
       "4                              16384       politics   \n",
       "..                               ...            ...   \n",
       "120  1000000000000000019884624838656          sport   \n",
       "121  1000000000000000019884624838656  entertainment   \n",
       "122  1000000000000000019884624838656  entertainment   \n",
       "123  1000000000000000019884624838656       business   \n",
       "124  1000000000000000019884624838656          sport   \n",
       "\n",
       "                                            transcript  \\\n",
       "0    Cuba winds back economic clock..Fidel Castro's...   \n",
       "1    Blair looks to election campaign..Tony Blair's...   \n",
       "2    New York rockers top talent poll..New York ele...   \n",
       "3    Terror suspects face house arrest..UK citizens...   \n",
       "4    'No more concessions' on terror..Charles Clark...   \n",
       "..                                                 ...   \n",
       "120  Paris promise raises Welsh hopes..Has there be...   \n",
       "121  Rapper Kanye West's shrewd soul..US hip-hop st...   \n",
       "122  Redford's vision of Sundance..Despite sporting...   \n",
       "123  Water firm Suez in Argentina row..A conflict b...   \n",
       "124  Lions blow to World Cup winners..British and I...   \n",
       "\n",
       "                                      original summary  \\\n",
       "0    Fidel Castro's decision to ban all cash transa...   \n",
       "1    There was little in terms of concrete proposal...   \n",
       "2    New York electro-rock group The Bravery have c...   \n",
       "3    British citizens are being included in the cha...   \n",
       "4    On Monday, MPs voted 272-219 in favour of the ...   \n",
       "..                                                 ...   \n",
       "120  But since they threw off the shackles against ...   \n",
       "121  Leaving his Chicago art school after only one ...   \n",
       "122  Redford wanted Sundance to be a platform for i...   \n",
       "123  The government has rejected the 60% rise and w...   \n",
       "124  But Woodward added: \"The key thing that I want...   \n",
       "\n",
       "                                               summary  \\\n",
       "0    The following is a summary of the announcement...   \n",
       "1    In this brief summary, we summarize the speech...   \n",
       "2    The narrator of this piece originally appeared...   \n",
       "3    The Home Secretary, Charles Clarke, has outlin...   \n",
       "4    The English Parliament passes the Prevention o...   \n",
       "..                                                 ...   \n",
       "120  This brief paper gives an update on the progre...   \n",
       "121  The Narrator informs the audience that Kany We...   \n",
       "122  Robert Redford is the founder and president of...   \n",
       "123  This paper describes a dispute between the Fre...   \n",
       "124  The Lions coach is preparing for the Rugby Wor...   \n",
       "\n",
       "                                                 rouge  \\\n",
       "0    [{'rouge-1': {'r': 0.38095238095238093, 'p': 0...   \n",
       "1    [{'rouge-1': {'r': 0.2874251497005988, 'p': 0....   \n",
       "2    [{'rouge-1': {'r': 0.4148148148148148, 'p': 0....   \n",
       "3    [{'rouge-1': {'r': 0.2710843373493976, 'p': 0....   \n",
       "4    [{'rouge-1': {'r': 0.3959731543624161, 'p': 0....   \n",
       "..                                                 ...   \n",
       "120  [{'rouge-1': {'r': 0.08695652173913043, 'p': 0...   \n",
       "121  [{'rouge-1': {'r': 0.09392265193370165, 'p': 0...   \n",
       "122  [{'rouge-1': {'r': 0.07096774193548387, 'p': 0...   \n",
       "123  [{'rouge-1': {'r': 0.07643312101910828, 'p': 0...   \n",
       "124  [{'rouge-1': {'r': 0.06832298136645963, 'p': 0...   \n",
       "\n",
       "                                            bert_score           bleu  \\\n",
       "0    (tensor([0.8857]), tensor([0.8723]), tensor([0...   2.181628e-01   \n",
       "1    (tensor([0.8664]), tensor([0.8317]), tensor([0...   9.503589e-02   \n",
       "2    (tensor([0.8524]), tensor([0.8609]), tensor([0...   1.759903e-01   \n",
       "3    (tensor([0.8730]), tensor([0.8468]), tensor([0...   5.694877e-02   \n",
       "4    (tensor([0.8890]), tensor([0.8720]), tensor([0...   1.677187e-01   \n",
       "..                                                 ...            ...   \n",
       "120  ([tensor(0.8620)], [tensor(0.8025)], [tensor(0...  4.201394e-157   \n",
       "121  ([tensor(0.8522)], [tensor(0.7918)], [tensor(0...  1.260210e-156   \n",
       "122  ([tensor(0.8724)], [tensor(0.8100)], [tensor(0...  8.507861e-159   \n",
       "123  ([tensor(0.8490)], [tensor(0.8013)], [tensor(0...   2.727474e-81   \n",
       "124  ([tensor(0.8783)], [tensor(0.8040)], [tensor(0...   2.898418e-83   \n",
       "\n",
       "     time_taken                                            grammar  \\\n",
       "0     19.899436  [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...   \n",
       "1     21.163034  [Match({'ruleId': 'EN_COMPOUNDS', 'message': '...   \n",
       "2     28.523080  [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...   \n",
       "3     24.956480  [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...   \n",
       "4     21.425882  [Match({'ruleId': 'MORFOLOGIK_RULE_EN_US', 'me...   \n",
       "..          ...                                                ...   \n",
       "120    9.210987                                                 []   \n",
       "121   15.023796  [Offset 39, length 4, Rule ID: MORFOLOGIK_RULE...   \n",
       "122    8.595882  [Offset 51, length 8, Rule ID: MORFOLOGIK_RULE...   \n",
       "123   16.457350  [Offset 106, length 9, Rule ID: MORFOLOGIK_RUL...   \n",
       "124    8.036771  [Offset 67, length 4, Rule ID: COMMA_COMPOUND_...   \n",
       "\n",
       "                                      readability  \n",
       "0    score: 13.012869198312234, grade_level: '13'  \n",
       "1     score: 9.665454545454548, grade_level: '10'  \n",
       "2    score: 11.396671501087742, grade_level: '11'  \n",
       "3    score: 11.617224157955867, grade_level: '12'  \n",
       "4      score: 9.484285714285715, grade_level: '9'  \n",
       "..                                            ...  \n",
       "120                           100 words required.  \n",
       "121                           100 words required.  \n",
       "122                           100 words required.  \n",
       "123                           100 words required.  \n",
       "124                           100 words required.  \n",
       "\n",
       "[125 rows x 13 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scores.to_excel(\"../Process/result/open_source_model_topics_comparison.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
